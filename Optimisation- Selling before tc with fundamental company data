import vectorbt as vbt
import yfinance as yf
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from scipy.optimize import least_squares
from sklearn.metrics import r2_score
import warnings
from lppls import lppls
import time
from vectorbt.portfolio.enums import SizeType
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from vectorbt.generic.plotting import Scatter
from multiprocessing import Pool, cpu_count
import os

warnings.filterwarnings('ignore')

model_window_param = 100
b_weight_param = 0.9
r2_weight_param = 0.1
signal_threshold_param = 70
buy_back_param = 40
max_searches_param = 20
days_after_tc_param = 40

# S&P 500 stocks
url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
tables = pd.read_html(url)
table = tables[0]
stocks = table['Symbol'].tolist()
stocks.remove('BRK.B')
stocks.remove('BF.B')
stocks.remove('GEHC')
stocks.remove('CEG')
stocks.remove('OTIS')
stocks.remove('CARR')
stocks.remove('OGN')
stocks100 = stocks[:100]
stocks10 = stocks[:10]

# Date range
start_date='2021-01-01'
end_date='2023-01-01'

#Helper functions
def loss(params, x, y):
    return func(x, *params) - y

def func(x, a, b, c):
    return a * np.exp(b * x) + c

start_time = time.time()

def load_quarterly_revenue(symbol, start_date):
    # Load the revenue data from the CSV file
    filename = f"{symbol}_quarterly_income_statement.csv"

    # Check if the file exists
    if not os.path.isfile(filename):
        print(f"File '{filename}' does not exist. Skipping {symbol}.")
        return None, None

    revenue_df = pd.read_csv(f"{symbol}_quarterly_income_statement.csv")

    # Convert the 'fiscalDateEnding' column to datetime
    revenue_df['fiscalDateEnding'] = pd.to_datetime(revenue_df['fiscalDateEnding'])

    # Set the index to the 'fiscalDateEnding' column
    revenue_df.set_index('fiscalDateEnding', inplace=True)

    # Sort the DataFrame by the index (i.e., 'fiscalDateEnding')
    revenue_df.sort_index(inplace=True)

    # Extract the 'totalRevenue' column
    revenue = revenue_df['totalRevenue']

    # Select the revenue data for the last quarter before start date
    last_revenue_before_start = revenue[revenue.index < start_date].last('Q').iloc[-1]

    # Select the revenue data for the same quarter in the previous year
    same_quarter_previous_year = revenue.asof(start_date - pd.DateOffset(years=1))

    if same_quarter_previous_year is None:
        raise ValueError(f"Not enough historical data for {symbol} to perform analysis starting from {start_date}")

    return last_revenue_before_start, same_quarter_previous_year

def compute_fetch_start_date(start_date, model_window):
    start_date_dt = pd.to_datetime(start_date)
    business_days = 0
    current_date = start_date_dt
    while business_days < model_window:
        current_date -= pd.DateOffset(days=1)
        if current_date.dayofweek < 5:  # It's a weekday
            business_days += 1
    return current_date.strftime("%Y-%m-%d")

fetch_start_date = compute_fetch_start_date(start_date, model_window_param)

# Downloading data
data = yf.download(['AAPL', 'MSFT'], start=fetch_start_date, end=end_date)['Close']
data = data.ffill().bfill()


# This is the actual indicator. Currently, it is based on the modelling of the log of 5-day moving average of the price data
# model_window days before the current date to an exponential curve. This modelling happens every 30 days (but can be modified).
# This is slightly arbitrary, but is just a starting point I chose, where the threshold depends on the multiple of the
# model's exponent and its r^2 value, ie how closely it resembles a faster-than-exponential growth. If the model meets the
# requirements, we fit it to the LPPL model. I have tried doing the normal curve_fit in this case, but that takes an 
# incredibly long time. However, there is an lppl library in Python which is very efficient and, having visualised it, 
# seems to be quite good at fitting. There is a parameter, max_searches, which can be adjusted to prevent overfitting etc.
# Then, if the predicted bubble burst is more than 14 days into the future, we buy the stock, and sell it 14 days before tc.
# We sell everything at the end of the window.
def analyze_stocks(data, model_window = model_window_param):
    df_b = pd.DataFrame()
    df_r2 = pd.DataFrame()
    all_dates = data.index.unique()
    for i in range(model_window, len(all_dates), 30): # here we loop over dates first
        date_b_values = []
        date_r2_values = []
        tickers = []
        for ticker in data.columns:
            df = data[ticker]
            if all_dates[i] not in df.index:
                continue
            historical_data = df.loc[all_dates[i-model_window]:all_dates[i]]
            historical_data = historical_data.rolling(window=5).mean().dropna()
            if len(historical_data) == 0:
                continue
            xdata = (historical_data.index - historical_data.index[0]).days.values
            ydata = historical_data.values
            log_ydata = np.log(ydata + 1).ravel()
            res = least_squares(loss, x0=(1, 0.01, 1), args=(xdata, log_ydata))
            fitted_y = func(xdata, *res.x).ravel()
            a, b, c = res.x
            r2 = r2_score(log_ydata, fitted_y)

            last_revenue_before_start, same_quarter_previous_year = load_quarterly_revenue(ticker, df.index[i])
            # Calculate inverse of percentage change in revenue
            revenue_change = (last_revenue_before_start - same_quarter_previous_year) / same_quarter_previous_year
            revenue_adjustment = 1 / (1 + revenue_change)
            # Adjust b and r2 values based on revenue growth
            b = b * revenue_adjustment

            date_b_values.append(b)
            date_r2_values.append(r2)
            tickers.append(ticker)
        scaler = StandardScaler()
        date_b_values_scaled = scaler.fit_transform(np.array(date_b_values).reshape(-1,1)).ravel()
        date_r2_values_scaled = scaler.fit_transform(np.array(date_r2_values).reshape(-1,1)).ravel()
        df_b_temp = pd.DataFrame(date_b_values_scaled, columns=[all_dates[i]], index=tickers).T
        df_r2_temp = pd.DataFrame(date_r2_values_scaled, columns=[all_dates[i]], index=tickers).T
        df_b = pd.concat([df_b, df_b_temp], axis=0)
        df_r2 = pd.concat([df_r2, df_r2_temp], axis=0)
    return df_b, df_r2



# Creating dataframes for b and r2 values
df_b_values, df_r2_values = analyze_stocks(data, model_window_param)
df_b_values = df_b_values.loc[~df_b_values.index.duplicated(keep='first')]
df_r2_values = df_r2_values.loc[~df_r2_values.index.duplicated(keep='first')]
df_b_values = df_b_values.reindex(data.index)
df_r2_values = df_r2_values.reindex(data.index)

initial_capital = 1000000

def exp_growth_and_r2(close,
                    df_b_values,
                    df_r2_values,
                    model_window=model_window_param,
                    signal_threshold=signal_threshold_param,
                    b_weight = b_weight_param,
                    r2_weight = r2_weight_param,
                    buy_back=buy_back_param,
                    max_searches=max_searches_param,
                    days_after_tc=days_after_tc_param):

    try:
        output = np.zeros(len(close))
        ticker = close.columns[0]
        for i in range(model_window, len(close), 30):

            if np.isnan(df_b_values.loc[close.index[i], ticker]) or np.isnan(df_r2_values.loc[close.index[i], ticker]):
                continue

            b_value = df_b_values.loc[close.index[i], ticker]
            r2_value = df_r2_values.loc[close.index[i], ticker]
            b_norm_value = 50 + (50 / 3) * b_value
            r2_norm_value = 50 + (50 / 3) * r2_value
            signal = b_norm_value * b_weight + r2_norm_value * r2_weight

            if signal > signal_threshold:
                historical_data = close.iloc[i-model_window:i]  # slice close price data for historical data
                time = [pd.Timestamp.toordinal(t) for t in historical_data.index]
                price = np.log(historical_data.values).ravel()
                observations = np.array([time, price])
                tcs = []
                for _ in range(7):
                    lppls_model = lppls.LPPLS(observations=observations)
                    tc, m, w, a, b, c1, c2, phi, O, D = lppls_model.fit(max_searches)
                    tcs.append(tc)
                tc = np.median(tcs)
                tc = tc - datetime.toordinal(historical_data.index[-1])

                if tc < buy_back:  # Sell the stock when a crash is predicted within 30 days
                    output[i] = -1
                    if (i + buy_back + days_after_tc) < len(output):  # Buy it back after 30 days
                        output[i + buy_back + days_after_tc] = 1

        output[-1] = -1  # Sell everything at the end of the window
        return output

    except Exception as e:
        print(f"Couldn't process stock. Error: {str(e)}")


# Creating the indicator
BuyIndicator = vbt.IndicatorFactory(
    input_names=['close', 'df_b_values', 'df_r2_values'],
    param_names=['model_window',
                'signal_threshold',
                'b_weight',
                'r2_weight',
                'days_after_tc',
                'max_searches',
                'buy_back'],
    output_names=['buy_indicator']
).from_apply_func(exp_growth_and_r2,
                model_window=model_window_param,
                signal_threshold=signal_threshold_param,
                b_weight = b_weight_param,
                r2_weight = r2_weight_param,
                buy_back=buy_back_param,
                max_searches=max_searches_param,
                days_after_tc=days_after_tc_param,
                keep_pd=True,
                per_column=True)

import itertools

param_space = {
    'model_window': [80, 90, 100],
    'signal_threshold': [70, 75, 80, 85],
    'buy_back': [30, 40, 50],
    'max_searches': [20],
    'days_after_tc': [30, 40]
}

b_r2_pairs = [(0.8, 0.2), (0.85, 0.15), (0.9, 0.1), (0.95, 0.05), (1, 0)]
param_combinations = list(itertools.product(*param_space.values()))

def process_params(params):
    global data
    for b, r2 in b_r2_pairs:
        params_dict = dict(zip(param_space.keys(), params))
        params_dict['b_weight'] = b
        params_dict['r2_weight'] = r2

        df_b_values, df_r2_values = analyze_stocks(data, params_dict['model_window'])
        df_b_values = df_b_values.loc[~df_b_values.index.duplicated(keep='first')]
        df_r2_values = df_r2_values.loc[~df_r2_values.index.duplicated(keep='first')]
        df_b_values = df_b_values.reindex(data.index)
        df_r2_values = df_r2_values.reindex(data.index)

        buy_indicator = BuyIndicator.run(data, df_b_values, df_r2_values, **params_dict)

        entry_signal = buy_indicator.buy_indicator == 1
        exit_signal = buy_indicator.buy_indicator == -1

        entry_signal.columns = entry_signal.columns.droplevel(['custom_model_window', 'custom_signal_threshold', 'custom_b_weight', 'custom_r2_weight', 'custom_buy_back', 'custom_max_searches', 'custom_days_after_tc'])

        data = data.loc[start_date:end_date]
        entry_signal = entry_signal.loc[start_date:end_date]
        exit_signal = exit_signal.loc[start_date:end_date]

        entry_signal.iloc[0] = True

        portfolio = vbt.Portfolio.from_signals(
            data,
            entry_signal,
            exit_signal,
            init_cash=initial_capital,
            fees=0.001
        )

        # Get portfolio statistics
        total_return = portfolio.total_return().iloc[-1]
        return (params_dict, total_return)

# Get the number of CPU cores available
n_cpu = cpu_count()

# Create a multiprocessing Pool
p = Pool(n_cpu)

# Use the pool to apply process_params to all parameter combinations in parallel
results = p.map(process_params, param_combinations)

p.close()
p.join()

# Find the parameter set with the highest total return
best_params, highest_total_return = max(results, key=lambda x: x[1])

print("Best parameters:")
print(best_params)

print("Highest total return:")
print(highest_total_return)

#Downloading S&P 500 data
hold_data = data
data = data.loc[start_date:end_date]
hold_portfolio = vbt.Portfolio.from_holding(hold_data, init_cash=initial_capital, fees=0.001)

benchmark_stats = hold_portfolio.stats()
print("\nBenchmark stats:")
print(benchmark_stats)

print(f"Sharpe ratio: {hold_portfolio.sharpe_ratio(group_by = True, freq='D')}")
print(f"Sortino ratio: {hold_portfolio.sortino_ratio(group_by = True, freq='D')}")

end_time = time.time()
print(f'Time taken: {end_time - start_time} seconds')

end_time = time.time()
print(f'Time taken: {end_time - start_time} seconds')
