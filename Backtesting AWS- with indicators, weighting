import vectorbt as vbt
import yfinance as yf
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from scipy.optimize import least_squares
from sklearn.metrics import r2_score
import warnings
from lppls import lppls
import time
from vectorbt.portfolio.enums import SizeType
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
warnings.filterwarnings('ignore')
# Downloading the list of tickers in the S&P 500, removing BRK.B and BF.B as these always cause issues. 
# stocks100 is simply the first 100 stocks (just used for testing)
url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
tables = pd.read_html(url)
table = tables[0]
stocks = table['Symbol'].tolist()
stocks.remove('BRK.B')
stocks.remove('BF.B')
stocks.remove('GEHC')
stocks100 = stocks[:100]
stocks10 = stocks[:10]
# The start and end dates for the backtest. Data will be fetched from a certain time before that for modelling purposes.
start_date='2020-01-01'
end_date='2023-06-24'
# Loss function to calculate value of r^2
def loss(params, x, y):
    return func(x, *params) - y
# Exponential function for modelling
def func(x, a, b, c):
    return a * np.exp(b * x) + c
# This is the actual indicator. Currently, it is based on the modelling of the log of 5-day moving average of the price data
# model_window days before the current date to an exponential curve. This modelling happens every 30 days (but can be modified).
# This is slightly arbitrary, but is just a starting point I chose, where the threshold depends on the multiple of the
# model's exponent and its r^2 value, ie how closely it resembles a faster-than-exponential growth. If the model meets the
# requirements, we fit it to the LPPL model. I have tried doing the normal curve_fit in this case, but that takes an 
# incredibly long time. However, there is an lppl library in Python which is very efficient and, having visualised it, 
# seems to be quite good at fitting. There is a parameter, max_searches, which can be adjusted to prevent overfitting etc.
# Then, if the predicted bubble burst is more than 14 days into the future, we buy the stock, and sell it 14 days before tc.
# We sell everything at the end of the window.
def analyze_stocks(data):
    model_window = 120
    df_b = pd.DataFrame()
    df_r2 = pd.DataFrame()
    
    for ticker in data.columns:
        b_values = []
        r2_values = []
        dates = []

        df = data[ticker]
        for i in range(model_window, len(df), 30):
            historical_data = df.iloc[i-model_window:i]
            historical_data = historical_data.rolling(window=5).mean().dropna()
            xdata = (historical_data.index - historical_data.index[0]).days.values
            ydata = historical_data.values
            log_ydata = np.log(ydata + 1).ravel()
            res = least_squares(loss, x0=(1, 0.01, 1), args=(xdata, log_ydata))
            fitted_y = func(xdata, *res.x).ravel()
            a, b, c = res.x
            r2 = r2_score(log_ydata, fitted_y)
            b_values.append(b)
            r2_values.append(r2)
            dates.append(df.index[i])
        
        df_b_temp = pd.DataFrame(b_values, columns=[ticker], index=dates)
        df_r2_temp = pd.DataFrame(r2_values, columns=[ticker], index=dates)
        df_b = pd.concat([df_b, df_b_temp], axis=1)
        df_r2 = pd.concat([df_r2, df_r2_temp], axis=1)
    
    scaler = StandardScaler() 
    df_b = pd.DataFrame(scaler.fit_transform(df_b), columns=df_b.columns, index=dates)
    df_r2 = pd.DataFrame(scaler.fit_transform(df_r2), columns=df_r2.columns, index=dates)
    return df_b, df_r2

fetch_start_date_dt = datetime.strptime(start_date, "%Y-%m-%d") - timedelta(days=90)
fetch_start_date = fetch_start_date_dt.strftime("%Y-%m-%d")
start_time = time.time()
data = yf.download(stocks10, start=fetch_start_date, end=end_date)['Close']
data = data.ffill().bfill()
df_b_values, df_r2_values = analyze_stocks(data)

df_b_values = df_b_values.loc[~df_b_values.index.duplicated(keep='first')]
df_r2_values = df_r2_values.loc[~df_r2_values.index.duplicated(keep='first')]
df_b_values = df_b_values.reindex(data.index)
df_r2_values = df_r2_values.reindex(data.index)

initial_capital = 1000000
def exp_growth_and_r2(close, model_window=120, signal_threshold=150, days_before_tc=7, max_searches=20):
    try:
        output = np.zeros(len(close))
        ticker = close.columns[0]
        for i in range(model_window, len(close), 30):
            # skip if b or r2 values are NaN
            if np.isnan(df_b_values.loc[close.index[i], ticker]) or np.isnan(df_r2_values.loc[close.index[i], ticker]):
                continue
            
            b_value = df_b_values.loc[close.index[i], ticker]
            r2_value = df_r2_values.loc[close.index[i], ticker]


            b_norm_value = 50 + (100 / 3) * b_value
            r2_norm_value = 50 + (100 / 3) * r2_value

            signal = b_norm_value * 0.8 + r2_norm_value * 0.2
            
            if signal > signal_threshold:
                historical_data = close.iloc[i-model_window:i]  # slice close price data for historical data
                time = [pd.Timestamp.toordinal(t) for t in historical_data.index]
                price = np.log(historical_data.values).ravel()
                observations = np.array([time, price])
                tcs = []
                for _ in range(5):
                    lppls_model = lppls.LPPLS(observations=observations)
                    tc, m, w, a, b, c1, c2, phi, O, D = lppls_model.fit(max_searches)
                    tcs.append(tc)
                tc = np.median(tcs)
                tc = tc - datetime.toordinal(historical_data.index[-1])
                
                if tc > 14:
                    output[i] = 1
                    if i+int(round(tc))-14 < len(output):
                        output[i+int(round(tc))-14]=-1
                
        output[-1] = -1
        return output
    except Exception as e:
        print(f"Couldn't process stock. Error: {str(e)}")
# Creating the indicator
BuyIndicator = vbt.IndicatorFactory(
    input_names=['close'],
    param_names=['model_window', 'signal_threshold', 'days_before_tc', 'max_searches'],
    output_names=['buy_indicator']
).from_apply_func(exp_growth_and_r2,
                model_window=90, 
                signal_threshold = 80, 
                days_before_tc=14, 
                max_searches=25,
                keep_pd=True,
                per_column=True)
# Running the data through our indicator- days when we buy will have a value of 1, and when we sell -1. 0 otherwise
buy_indicator = BuyIndicator.run(data, model_window=120, signal_threshold = 75, days_before_tc=7, max_searches=20)
entry_signal = buy_indicator.buy_indicator == 1
exit_signal = buy_indicator.buy_indicator == -1
# Dropping some extra columns to make it match data
entry_signal.columns = entry_signal.columns.droplevel(['custom_model_window', 'custom_signal_threshold', 'custom_days_before_tc', 'custom_max_searches'])
# Cropping the data for the backtest to make it faster
data = data.loc[start_date:end_date]
entry_signal = entry_signal.loc[start_date:end_date]
exit_signal = exit_signal.loc[start_date:end_date]
# # Assigning an equal proportion of the capital to each stock, investing it all when we get an entr signal
# Convert entry and exit signals to target weights
target_weights = entry_signal.astype(float)
target_weights[exit_signal] = 0.0

# Divide each row by its sum to ensure that the target weights of all stocks sum up to 100% on each day
target_weights = target_weights.div(target_weights.sum(axis=1), axis=0)

# Replace NaN values (days with no trades) with 0
target_weights.fillna(0, inplace=True)

# Creating the portfolio
portfolio = vbt.Portfolio.from_orders(
    data,
    target_weights,
    size_type=SizeType.TargetPercent,
    close_first=True,  # To close existing positions before opening new ones
    fees=0.001,
    init_cash=initial_capital,
)
# Printing stats- can add extra info about the portfolio at this point
print(portfolio.stats(metrics = ["start", 
                                 "end", 
                                 "period", 
                                 "start_value", 
                                 "end_value", 
                                 "total_return", 
                                 "benchmark_return",
                                 "max_gross_exposure", 
                                 "total_fees_paid", 
                                 "max_dd", 
                                 "total_trades", 
                                 "win_rate", 
                                 "profit_factor", 
                                 "expectancy", 
                                 "sharpe_ratio", 
                                 "sortino_ratio"], group_by = True))
#total_portfolio_value = portfolio.value().sum(axis=1)
#total_portfolio_value.vbt.plot().show()
sp500_data = yf.download('^GSPC', start=start_date, end=end_date)['Adj Close']
# Calculate the return for S&P 500 over the entire period
sp500_returns = sp500_data.pct_change().dropna()
# Calculate the total return of S&P 500
total_sp500_return = (sp500_returns + 1).prod() - 1
# Print the total return
print(f"S&P 500 Total Return: {total_sp500_return * 100:.2f}%")
exposure = portfolio.gross_exposure(group_by=True)
# Get the returns of the portfolio for each day
returns = portfolio.returns(group_by=True)
# Adjust the returns by the gross exposure
adjusted_returns = returns / exposure
# Handle cases where exposure is zero (to avoid division by zero)
adjusted_returns = adjusted_returns.replace([np.inf, -np.inf], np.nan)
adjusted_total_returns = (adjusted_returns + 1).cumprod() - 1
final_adjusted_total_return = adjusted_total_returns.iloc[-2]
print(f"Final adjusted total return: {final_adjusted_total_return * 100:.2f}%")
print(f"Sharpe ratio: {portfolio.sharpe_ratio(group_by = True, freq='D')}")
print(f"Sortino ratio: {portfolio.sortino_ratio(group_by = True, freq='D')}")
end_time = time.time()
print(f'Time taken: {end_time - start_time} seconds')
